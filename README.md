# MetaLLM - Unified Go SDK for Large Language Models

[![Build Status][build-status-svg]][build-status-url]
[![Lint Status][lint-status-svg]][lint-status-url]
[![Go Report Card][goreport-svg]][goreport-url]
[![Docs][docs-godoc-svg]][docs-godoc-url]
[![License][license-svg]][license-url]

MetaLLM is a unified Go SDK that provides a consistent interface for interacting with multiple Large Language Model (LLM) providers including OpenAI, Anthropic (Claude), Google Gemini, X.AI (Grok), and Ollama. It implements the Chat Completions API pattern and offers both synchronous and streaming capabilities. Additional providers like AWS Bedrock are available as [external modules](#external-providers).

## ‚ú® Features

- **üîå Multi-Provider Support**: OpenAI, Anthropic (Claude), Google Gemini, X.AI (Grok), Ollama, plus [external providers](#external-providers) (AWS Bedrock, etc.)
- **üéØ Unified API**: Same interface across all providers
- **üì° Streaming Support**: Real-time response streaming for all providers
- **üß† Conversation Memory**: Persistent conversation history using Key-Value Stores
- **üìä Observability Hooks**: Extensible hooks for tracing, logging, and metrics without modifying core library
- **üîÑ Retry with Backoff**: Automatic retries for transient failures (rate limits, 5xx errors)
- **üß™ Comprehensive Testing**: Unit tests, integration tests, and mock implementations included
- **üîß Extensible**: Easy to add new LLM providers
- **üì¶ Modular**: Provider-specific implementations in separate packages
- **üèóÔ∏è Reference Architecture**: Internal providers serve as reference implementations for external providers
- **üîå 3rd Party Friendly**: External providers can be injected without modifying core library
- **‚ö° Type Safe**: Full Go type safety with comprehensive error handling

## üèóÔ∏è Architecture

MetaLLM uses a clean, modular architecture that separates concerns and enables easy extensibility:

```
metallm/
‚îú‚îÄ‚îÄ client.go            # Main ChatClient wrapper
‚îú‚îÄ‚îÄ providers.go         # Factory functions for built-in providers
‚îú‚îÄ‚îÄ types.go             # Type aliases for backward compatibility
‚îú‚îÄ‚îÄ memory.go            # Conversation memory management
‚îú‚îÄ‚îÄ observability.go     # ObservabilityHook interface for tracing/logging/metrics
‚îú‚îÄ‚îÄ errors.go            # Unified error handling
‚îú‚îÄ‚îÄ *_test.go            # Comprehensive unit tests
‚îú‚îÄ‚îÄ provider/            # üéØ Public interface package for external providers
‚îÇ   ‚îú‚îÄ‚îÄ interface.go     # Provider interface that all providers must implement
‚îÇ   ‚îî‚îÄ‚îÄ types.go         # Unified request/response types
‚îú‚îÄ‚îÄ providers/           # üì¶ Individual provider packages (reference implementations)
‚îÇ   ‚îú‚îÄ‚îÄ openai/          # OpenAI implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.go    # HTTP client
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.go     # OpenAI-specific types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter.go   # provider.Provider implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *_test.go    # Provider tests
‚îÇ   ‚îú‚îÄ‚îÄ anthropic/       # Anthropic implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.go # HTTP client (SSE streaming)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.go     # Anthropic-specific types
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter.go   # provider.Provider implementation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *_test.go    # Provider and integration tests
‚îÇ   ‚îú‚îÄ‚îÄ gemini/          # Google Gemini implementation
‚îÇ   ‚îú‚îÄ‚îÄ xai/             # X.AI Grok implementation
‚îÇ   ‚îî‚îÄ‚îÄ ollama/          # Ollama implementation
‚îî‚îÄ‚îÄ testing/             # üß™ Test utilities
    ‚îî‚îÄ‚îÄ mock_kvs.go      # Mock KVS for memory testing
```

### Key Architecture Benefits

- **üéØ Public Interface**: The `provider` package exports the `Provider` interface that external packages can implement
- **üèóÔ∏è Reference Implementation**: Internal providers follow the exact same structure that external providers should use
- **üîå Direct Injection**: External providers are injected via `ClientConfig.CustomProvider` without modifying core code
- **üì¶ Modular Design**: Each provider is self-contained with its own HTTP client, types, and adapter
- **üß™ Testable**: Clean interfaces that can be easily mocked and tested
- **üîß Extensible**: New providers can be added without touching existing code
- **‚ö° Native Implementation**: Uses standard `net/http` for direct API communication (no official SDK dependencies)

## üöÄ Quick Start

### Installation

```bash
go get github.com/grokify/metallm
```

### Basic Usage

```go
package main

import (
    "context"
    "fmt"
    "log"
    
    "github.com/grokify/metallm"
)

func main() {
    // Create a client for OpenAI
    client, err := metallm.NewClient(metallm.ClientConfig{
        Provider: metallm.ProviderNameOpenAI,
        APIKey:   "your-openai-api-key",
    })
    if err != nil {
        log.Fatal(err)
    }
    defer client.Close()

    // Create a chat completion request
    response, err := client.CreateChatCompletion(context.Background(), &metallm.ChatCompletionRequest{
        Model: metallm.ModelGPT4o,
        Messages: []metallm.Message{
            {
                Role:    metallm.RoleUser,
                Content: "Hello! How can you help me today?",
            },
        },
        MaxTokens:   &[]int{150}[0],
        Temperature: &[]float64{0.7}[0],
    })
    if err != nil {
        log.Fatal(err)
    }

    fmt.Printf("Response: %s\n", response.Choices[0].Message.Content)
    fmt.Printf("Tokens used: %d\n", response.Usage.TotalTokens)
}
```

## üîß Supported Providers

### OpenAI

- **Models**: GPT-5, GPT-4.1, GPT-4o, GPT-4o-mini, GPT-4-turbo, GPT-3.5-turbo
- **Features**: Chat completions, streaming, function calling

```go
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "your-openai-api-key",
    BaseURL:  "https://api.openai.com/v1", // optional
})
```

### Anthropic (Claude)

- **Models**: Claude-Opus-4.1, Claude-Opus-4, Claude-Sonnet-4, Claude-3.7-Sonnet, Claude-3.5-Haiku, Claude-3-Opus, Claude-3-Sonnet, Claude-3-Haiku
- **Features**: Chat completions, streaming, system message support

```go
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameAnthropic,
    APIKey:   "your-anthropic-api-key",
    BaseURL:  "https://api.anthropic.com", // optional
})
```

### Google Gemini

- **Models**: Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-1.5-Pro, Gemini-1.5-Flash
- **Features**: Chat completions, streaming

```go
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameGemini,
    APIKey:   "your-gemini-api-key",
})
```

### AWS Bedrock (External Provider)

AWS Bedrock is available as an external module to avoid pulling AWS SDK dependencies for users who don't need it.

```bash
go get github.com/grokify/metallm-bedrock
```

```go
import (
    "github.com/grokify/metallm"
    "github.com/grokify/metallm-bedrock"
)

// Create the Bedrock provider
bedrockProvider, err := bedrock.NewProvider("us-east-1")
if err != nil {
    log.Fatal(err)
}

// Use it with metallm via CustomProvider
client, err := metallm.NewClient(metallm.ClientConfig{
    CustomProvider: bedrockProvider,
})
```

See [External Providers](#external-providers) for more details.

### X.AI (Grok)

- **Models**: Grok-4.1-Fast (Reasoning/Non-Reasoning), Grok-4 (0709), Grok-4-Fast (Reasoning/Non-Reasoning), Grok-Code-Fast, Grok-3, Grok-3-Mini, Grok-2, Grok-2-Vision
- **Features**: Chat completions, streaming, OpenAI-compatible API, 2M context window (4.1/4-Fast models)

```go
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameXAI,
    APIKey:   "your-xai-api-key",
    BaseURL:  "https://api.x.ai/v1", // optional
})
```

### Ollama (Local Models)

- **Models**: Llama 3, Mistral, CodeLlama, Gemma, Qwen2.5, DeepSeek-Coder
- **Features**: Local inference, no API keys required, optimized for Apple Silicon

```go
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOllama,
    BaseURL:  "http://localhost:11434", // default Ollama endpoint
})
```

## üîå External Providers

Some providers with heavy SDK dependencies are available as separate modules to keep the core library lightweight. These are injected via `ClientConfig.CustomProvider`.

| Provider | Module | Why External |
|----------|--------|--------------|
| AWS Bedrock | [github.com/grokify/metallm-bedrock](https://github.com/grokify/metallm-bedrock) | AWS SDK v2 adds 17+ transitive dependencies |

### Using External Providers

```go
import (
    "github.com/grokify/metallm"
    "github.com/grokify/metallm-bedrock"  // or your custom provider
)

// Create the external provider
provider, err := bedrock.NewProvider("us-east-1")
if err != nil {
    log.Fatal(err)
}

// Inject via CustomProvider
client, err := metallm.NewClient(metallm.ClientConfig{
    CustomProvider: provider,
})
```

### Creating Your Own External Provider

External providers implement the `provider.Provider` interface:

```go
import "github.com/grokify/metallm/provider"

type MyProvider struct{}

func (p *MyProvider) Name() string { return "myprovider" }
func (p *MyProvider) Close() error { return nil }

func (p *MyProvider) CreateChatCompletion(ctx context.Context, req *provider.ChatCompletionRequest) (*provider.ChatCompletionResponse, error) {
    // Your implementation
}

func (p *MyProvider) CreateChatCompletionStream(ctx context.Context, req *provider.ChatCompletionRequest) (provider.ChatCompletionStream, error) {
    // Your streaming implementation
}
```

See the [metallm-bedrock](https://github.com/grokify/metallm-bedrock) source code as a reference implementation.

## üì° Streaming Example

```go
stream, err := client.CreateChatCompletionStream(context.Background(), &metallm.ChatCompletionRequest{
    Model: metallm.ModelGPT4o,
    Messages: []metallm.Message{
        {
            Role:    metallm.RoleUser,
            Content: "Tell me a short story about AI.",
        },
    },
    MaxTokens:   &[]int{200}[0],
    Temperature: &[]float64{0.8}[0],
})
if err != nil {
    log.Fatal(err)
}
defer stream.Close()

fmt.Print("AI Response: ")
for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        log.Fatal(err)
    }
    
    if len(chunk.Choices) > 0 && chunk.Choices[0].Delta != nil {
        fmt.Print(chunk.Choices[0].Delta.Content)
    }
}
fmt.Println()
```

## üß† Conversation Memory

MetaLLM supports persistent conversation memory using any Key-Value Store that implements the [Sogo KVS interface](https://github.com/grokify/sogo/blob/master/database/kvs/definitions.go). This enables multi-turn conversations that persist across application restarts.

### Memory Configuration

```go
// Configure memory settings
memoryConfig := metallm.MemoryConfig{
    MaxMessages: 50,                    // Keep last 50 messages per session
    TTL:         24 * time.Hour,       // Messages expire after 24 hours
    KeyPrefix:   "myapp:conversations", // Custom key prefix
}

// Create client with memory (using Redis, DynamoDB, etc.)
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider:     metallm.ProviderNameOpenAI,
    APIKey:       "your-api-key",
    Memory:       kvsClient,          // Your KVS implementation
    MemoryConfig: &memoryConfig,
})
```

### Memory-Aware Completions

```go
// Create a session with system message
err = client.CreateConversationWithSystemMessage(ctx, "user-123", 
    "You are a helpful assistant that remembers our conversation history.")

// Use memory-aware completion - automatically loads conversation history
response, err := client.CreateChatCompletionWithMemory(ctx, "user-123", &metallm.ChatCompletionRequest{
    Model: metallm.ModelGPT4o,
    Messages: []metallm.Message{
        {Role: metallm.RoleUser, Content: "What did we discuss last time?"},
    },
    MaxTokens: &[]int{200}[0],
})

// The response will include context from previous conversations in this session
```

### Memory Management

```go
// Load conversation history
conversation, err := client.LoadConversation(ctx, "user-123")

// Get just the messages
messages, err := client.GetConversationMessages(ctx, "user-123")

// Manually append messages
err = client.AppendMessage(ctx, "user-123", metallm.Message{
    Role:    metallm.RoleUser,
    Content: "Remember this important fact: I prefer JSON responses.",
})

// Delete conversation
err = client.DeleteConversation(ctx, "user-123")
```

### KVS Backend Support

Memory works with any KVS implementation:
- **Redis**: For high-performance, distributed memory
- **DynamoDB**: For AWS-native storage
- **In-Memory**: For testing and development
- **Custom**: Any implementation of the Sogo KVS interface

```go
// Example with Redis (using a hypothetical Redis KVS implementation)
redisKVS := redis.NewKVSClient("localhost:6379")
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "your-key",
    Memory:   redisKVS,
})
```

## üìä Observability Hooks

MetaLLM supports observability hooks that allow you to add tracing, logging, and metrics to LLM calls without modifying the core library. This is useful for integrating with observability platforms like OpenTelemetry, Datadog, or custom monitoring solutions.

### ObservabilityHook Interface

```go
// LLMCallInfo provides metadata about the LLM call
type LLMCallInfo struct {
    CallID       string    // Unique identifier for correlating BeforeRequest/AfterResponse
    ProviderName string    // e.g., "openai", "anthropic"
    StartTime    time.Time // When the call started
}

// ObservabilityHook allows external packages to observe LLM calls
type ObservabilityHook interface {
    // BeforeRequest is called before each LLM call.
    // Returns a new context for trace/span propagation.
    BeforeRequest(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest) context.Context

    // AfterResponse is called after each LLM call completes (success or failure).
    AfterResponse(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, resp *provider.ChatCompletionResponse, err error)

    // WrapStream wraps a stream for observability of streaming responses.
    // Note: AfterResponse is only called if stream creation fails. For streaming
    // completion timing, handle Close() or EOF detection in your wrapper.
    WrapStream(ctx context.Context, info LLMCallInfo, req *provider.ChatCompletionRequest, stream provider.ChatCompletionStream) provider.ChatCompletionStream
}
```

### Basic Usage

```go
// Create a simple logging hook
type LoggingHook struct{}

func (h *LoggingHook) BeforeRequest(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest) context.Context {
    log.Printf("[%s] LLM call started: provider=%s model=%s", info.CallID, info.ProviderName, req.Model)
    return ctx
}

func (h *LoggingHook) AfterResponse(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest, resp *metallm.ChatCompletionResponse, err error) {
    duration := time.Since(info.StartTime)
    if err != nil {
        log.Printf("[%s] LLM call failed: provider=%s duration=%v error=%v", info.CallID, info.ProviderName, duration, err)
    } else {
        log.Printf("[%s] LLM call completed: provider=%s duration=%v tokens=%d", info.CallID, info.ProviderName, duration, resp.Usage.TotalTokens)
    }
}

func (h *LoggingHook) WrapStream(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest, stream metallm.ChatCompletionStream) metallm.ChatCompletionStream {
    return stream // Return unwrapped for simple logging, or wrap for streaming metrics
}

// Use the hook when creating a client
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider:          metallm.ProviderNameOpenAI,
    APIKey:            "your-api-key",
    ObservabilityHook: &LoggingHook{},
})
```

### OpenTelemetry Integration Example

```go
type OTelHook struct {
    tracer trace.Tracer
}

func (h *OTelHook) BeforeRequest(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest) context.Context {
    ctx, span := h.tracer.Start(ctx, "llm.chat_completion",
        trace.WithAttributes(
            attribute.String("llm.provider", info.ProviderName),
            attribute.String("llm.model", req.Model),
        ),
    )
    return ctx
}

func (h *OTelHook) AfterResponse(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest, resp *metallm.ChatCompletionResponse, err error) {
    span := trace.SpanFromContext(ctx)
    defer span.End()

    if err != nil {
        span.RecordError(err)
        span.SetStatus(codes.Error, err.Error())
    } else if resp != nil {
        span.SetAttributes(
            attribute.Int("llm.tokens.total", resp.Usage.TotalTokens),
            attribute.Int("llm.tokens.prompt", resp.Usage.PromptTokens),
            attribute.Int("llm.tokens.completion", resp.Usage.CompletionTokens),
        )
    }
}

func (h *OTelHook) WrapStream(ctx context.Context, info metallm.LLMCallInfo, req *metallm.ChatCompletionRequest, stream metallm.ChatCompletionStream) metallm.ChatCompletionStream {
    return &observableStream{stream: stream, ctx: ctx, info: info}
}
```

### Key Benefits

- **Non-Invasive**: Add observability without modifying core library code
- **Provider Agnostic**: Works with all LLM providers (OpenAI, Anthropic, Gemini, etc.)
- **Streaming Support**: Wrap streams to observe streaming responses
- **Context Propagation**: Pass trace context through the entire call chain
- **Flexible**: Implement only the methods you need; all are called if the hook is set

## üîÑ Provider Switching

The unified interface makes it easy to switch between providers:

```go
// Same request works with any provider
request := &metallm.ChatCompletionRequest{
    Model: metallm.ModelGPT4o, // or metallm.ModelClaude3Sonnet, etc.
    Messages: []metallm.Message{
        {Role: metallm.RoleUser, Content: "Hello, world!"},
    },
    MaxTokens: &[]int{100}[0],
}

// OpenAI
openaiClient, _ := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "openai-key",
})

// Anthropic
anthropicClient, _ := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameAnthropic,
    APIKey:   "anthropic-key",
})

// Gemini
geminiClient, _ := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameGemini,
    APIKey:   "gemini-key",
})

// Same API call for all providers
response1, _ := openaiClient.CreateChatCompletion(ctx, request)
response2, _ := anthropicClient.CreateChatCompletion(ctx, request)
response3, _ := geminiClient.CreateChatCompletion(ctx, request)
```

## üß™ Testing

MetaLLM includes a comprehensive test suite with both unit tests and integration tests.

### Running Tests

```bash
# Run all unit tests (no API keys required)
go test ./... -short

# Run with coverage
go test ./... -short -cover

# Run integration tests (requires API keys)
ANTHROPIC_API_KEY=your-key go test ./providers/anthropic -v
OPENAI_API_KEY=your-key go test ./providers/openai -v
XAI_API_KEY=your-key go test ./providers/xai -v

# Run all tests including integration
ANTHROPIC_API_KEY=your-key OPENAI_API_KEY=your-key XAI_API_KEY=your-key go test ./... -v
```

### Test Coverage

- **Unit Tests**: Mock-based tests that run without external dependencies
- **Integration Tests**: Real API tests that skip gracefully when API keys are not set
- **Memory Tests**: Comprehensive conversation memory management tests
- **Provider Tests**: Adapter logic, message conversion, and streaming tests

### Writing Tests

The clean interface design makes testing straightforward:

```go
// Mock the Provider interface for testing
type mockProvider struct{}

func (m *mockProvider) CreateChatCompletion(ctx context.Context, req *metallm.ChatCompletionRequest) (*metallm.ChatCompletionResponse, error) {
    return &metallm.ChatCompletionResponse{
        Choices: []metallm.ChatCompletionChoice{
            {
                Message: metallm.Message{
                    Role:    metallm.RoleAssistant,
                    Content: "Mock response",
                },
            },
        },
    }, nil
}

func (m *mockProvider) CreateChatCompletionStream(ctx context.Context, req *metallm.ChatCompletionRequest) (metallm.ChatCompletionStream, error) {
    return nil, nil
}

func (m *mockProvider) Close() error { return nil }
func (m *mockProvider) Name() string { return "mock" }
```

### Conditional Integration Tests

Integration tests automatically skip when API keys are not available:

```go
func TestAnthropicIntegration_Streaming(t *testing.T) {
    apiKey := os.Getenv("ANTHROPIC_API_KEY")
    if apiKey == "" {
        t.Skip("Skipping integration test: ANTHROPIC_API_KEY not set")
    }
    // Test code here...
}
```

### Mock KVS for Memory Testing

MetaLLM provides a mock KVS implementation for testing memory functionality:

```go
import metallmtest "github.com/grokify/metallm/testing"

// Create mock KVS for testing
mockKVS := metallmtest.NewMockKVS()

client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "test-key",
    Memory:   mockKVS,
})
```

## üìö Examples

The repository includes comprehensive examples:

- **Basic Usage**: Simple chat completions with each provider
- **Streaming**: Real-time response handling
- **Conversation**: Multi-turn conversations with context
- **Memory Demo**: Persistent conversation memory with KVS backend
- **Architecture Demo**: Overview of the provider architecture
- **Custom Provider**: How to create and use 3rd party providers

Run examples:
```bash
go run examples/basic/main.go
go run examples/streaming/main.go
go run examples/anthropic_streaming/main.go
go run examples/conversation/main.go
go run examples/memory_demo/main.go
go run examples/providers_demo/main.go
go run examples/xai/main.go
go run examples/ollama/main.go
go run examples/ollama_streaming/main.go
go run examples/gemini/main.go
go run examples/custom_provider/main.go
```

## üîß Configuration

### Environment Variables
- `OPENAI_API_KEY`: Your OpenAI API key
- `ANTHROPIC_API_KEY`: Your Anthropic API key
- `GEMINI_API_KEY`: Your Google Gemini API key
- `XAI_API_KEY`: Your X.AI API key

### Advanced Configuration

```go
config := metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "your-api-key",
    BaseURL:  "https://custom-endpoint.com/v1",
    Extra: map[string]any{
        "timeout": 60, // Custom provider-specific settings
    },
}
```

### Logging Configuration

MetaLLM supports injectable logging via Go's standard `log/slog` package. If no logger is provided, a null logger is used (no output).

```go
import (
    "log/slog"
    "os"

    "github.com/grokify/metallm"
)

// Use a custom logger
logger := slog.New(slog.NewJSONHandler(os.Stderr, &slog.HandlerOptions{
    Level: slog.LevelDebug,
}))

client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   "your-api-key",
    Logger:   logger, // Optional: defaults to null logger if not provided
})

// Access the logger if needed
client.Logger().Info("client initialized", slog.String("provider", "openai"))
```

The logger is used internally for non-critical errors (e.g., memory save failures) that shouldn't interrupt the main request flow.

### Context-Aware Logging

MetaLLM supports request-scoped logging via context. This allows you to attach trace IDs, user IDs, or other request-specific attributes to all log output within a request:

```go
import (
    "log/slog"

    "github.com/grokify/metallm"
    "github.com/grokify/mogo/log/slogutil"
)

// Create a request-scoped logger with trace/user context
reqLogger := slog.Default().With(
    slog.String("trace_id", traceID),
    slog.String("user_id", userID),
    slog.String("request_id", requestID),
)

// Attach logger to context
ctx = slogutil.ContextWithLogger(ctx, reqLogger)

// All internal logging will now include trace_id, user_id, and request_id
response, err := client.CreateChatCompletionWithMemory(ctx, sessionID, req)
```

The context-aware logger is retrieved using `slogutil.LoggerFromContext(ctx, fallback)`, which returns the context logger if present, or falls back to the client's configured logger.

### Retry with Backoff

MetaLLM supports automatic retries for transient failures (rate limits, 5xx errors) via a custom HTTP client. This uses the `retryhttp` package from `github.com/grokify/mogo`.

```go
import (
    "net/http"
    "time"

    "github.com/grokify/metallm"
    "github.com/grokify/mogo/net/http/retryhttp"
)

// Create retry transport with exponential backoff
rt := retryhttp.NewWithOptions(
    retryhttp.WithMaxRetries(5),                           // Max 5 retries
    retryhttp.WithInitialBackoff(500 * time.Millisecond),  // Start with 500ms
    retryhttp.WithMaxBackoff(30 * time.Second),            // Cap at 30s
    retryhttp.WithOnRetry(func(attempt int, req *http.Request, resp *http.Response, err error, backoff time.Duration) {
        log.Printf("Retry attempt %d, waiting %v", attempt, backoff)
    }),
)

// Create client with retry-enabled HTTP client
client, err := metallm.NewClient(metallm.ClientConfig{
    Provider: metallm.ProviderNameOpenAI,
    APIKey:   os.Getenv("OPENAI_API_KEY"),
    HTTPClient: &http.Client{
        Transport: rt,
        Timeout:   2 * time.Minute, // Allow time for retries
    },
})
```

**Retry Transport Features:**
| Feature | Default | Description |
|---------|---------|-------------|
| Max Retries | 3 | Maximum retry attempts |
| Initial Backoff | 1s | Starting backoff duration |
| Max Backoff | 30s | Cap on backoff duration |
| Backoff Multiplier | 2.0 | Exponential growth factor |
| Jitter | 10% | Randomness to prevent thundering herd |
| Retryable Status Codes | 429, 500, 502, 503, 504 | Rate limits + 5xx errors |

**Additional Options:**
- `WithRetryableStatusCodes(codes)` - Custom status codes to retry
- `WithShouldRetry(fn)` - Custom retry decision function
- `WithLogger(logger)` - Structured logging for retry events
- Respects `Retry-After` headers from API responses

**Provider Support:** Works with OpenAI, Anthropic, X.AI, and Ollama providers. Gemini and Bedrock use SDK clients with their own retry mechanisms.

## üèóÔ∏è Adding New Providers

### üéØ 3rd Party Providers (Recommended)

External packages can create providers without modifying the core library. This is the recommended approach for most use cases:

#### Step 1: Create Your Provider Package

```go
// In your external package (e.g., github.com/yourname/metallm-gemini)
package gemini

import (
    "context"
    "github.com/grokify/metallm/provider"
)

// Step 1: HTTP Client (like providers/openai/openai.go)
type Client struct {
    apiKey string
    // your HTTP client implementation
}

func New(apiKey string) *Client {
    return &Client{apiKey: apiKey}
}

// Step 2: Provider Adapter (like providers/openai/adapter.go)
type Provider struct {
    client *Client
}

func NewProvider(apiKey string) provider.Provider {
    return &Provider{client: New(apiKey)}
}

func (p *Provider) CreateChatCompletion(ctx context.Context, req *provider.ChatCompletionRequest) (*provider.ChatCompletionResponse, error) {
    // Convert provider.ChatCompletionRequest to your API format
    // Make HTTP call via p.client
    // Convert response back to provider.ChatCompletionResponse
}

func (p *Provider) CreateChatCompletionStream(ctx context.Context, req *provider.ChatCompletionRequest) (provider.ChatCompletionStream, error) {
    // Your streaming implementation
}

func (p *Provider) Close() error { return p.client.Close() }
func (p *Provider) Name() string { return "gemini" }
```

#### Step 2: Use Your Provider

```go
import (
    "github.com/grokify/metallm"
    "github.com/yourname/metallm-gemini"
)

func main() {
    // Create your custom provider
    customProvider := gemini.NewProvider("your-api-key")

    // Inject it directly into metallm - no core modifications needed!
    client, err := metallm.NewClient(metallm.ClientConfig{
        CustomProvider: customProvider,
    })

    // Use the same metallm API
    response, err := client.CreateChatCompletion(ctx, &metallm.ChatCompletionRequest{
        Model: "gemini-pro",
        Messages: []metallm.Message{{Role: metallm.RoleUser, Content: "Hello!"}},
    })
}
```

### üîß Built-in Providers (For Core Contributors)

To add a built-in provider to the core library, follow the same structure as existing providers:

1. **Create Provider Package**: `providers/newprovider/`
   - `newprovider.go` - HTTP client implementation
   - `types.go` - Provider-specific request/response types
   - `adapter.go` - `provider.Provider` interface implementation

2. **Update Core Files**:
   - Add factory function in `providers.go`
   - Add provider constant in `constants.go`
   - Add model constants if needed

3. **Reference Implementation**: Look at any existing provider (e.g., `providers/openai/`) as they all follow the exact same pattern that external providers should use

### üéØ Why This Architecture?

- **üîå No Core Changes**: External providers don't require modifying the core library
- **üèóÔ∏è Reference Pattern**: Internal providers demonstrate the exact structure external providers should follow
- **üß™ Easy Testing**: Both internal and external providers use the same `provider.Provider` interface
- **üì¶ Self-Contained**: Each provider manages its own HTTP client, types, and adapter logic
- **üîß Direct Injection**: Clean dependency injection via `ClientConfig.CustomProvider`

## üìä Model Support

| Provider | Models | Features |
|----------|--------|----------|
| OpenAI | GPT-5, GPT-4.1, GPT-4o, GPT-4o-mini, GPT-4-turbo, GPT-3.5-turbo | Chat, Streaming, Functions |
| Anthropic | Claude-Opus-4.1, Claude-Opus-4, Claude-Sonnet-4, Claude-3.7-Sonnet, Claude-3.5-Haiku | Chat, Streaming, System messages |
| Gemini | Gemini-2.5-Pro, Gemini-2.5-Flash, Gemini-1.5-Pro, Gemini-1.5-Flash | Chat, Streaming |
| X.AI | Grok-4.1-Fast, Grok-4, Grok-4-Fast, Grok-Code-Fast, Grok-3, Grok-3-Mini, Grok-2 | Chat, Streaming, 2M context, Tool calling |
| Ollama | Llama 3, Mistral, CodeLlama, Gemma, Qwen2.5, DeepSeek-Coder | Chat, Streaming, Local inference |
| Bedrock* | Claude models, Titan models | Chat, Multiple model families |

*Available as [external module](https://github.com/grokify/metallm-bedrock)

## üö® Error Handling

MetaLLM provides comprehensive error handling with provider-specific context:

```go
response, err := client.CreateChatCompletion(ctx, request)
if err != nil {
    if apiErr, ok := err.(*metallm.APIError); ok {
        fmt.Printf("Provider: %s, Status: %d, Message: %s\n", 
            apiErr.Provider, apiErr.StatusCode, apiErr.Message)
    }
}
```

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests to ensure everything works:
   ```bash
   go test ./... -short        # Run unit tests
   go build ./...              # Verify build
   go vet ./...                # Run static analysis
   ```
5. Commit your changes (`git commit -m 'Add some amazing feature'`)
6. Push to the branch (`git push origin feature/amazing-feature`)
7. Open a Pull Request

### Adding Tests

When contributing new features:
- Add unit tests for core logic
- Add integration tests for provider implementations (with API key checks)
- Ensure tests pass without API keys using `-short` flag
- Mock external dependencies when possible

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üîó Related Projects

- [Anthropic Go SDK](https://github.com/anthropics/anthropic-sdk-go) - Official Anthropic SDK
- [OpenAI Go SDK](https://github.com/openai/openai-go) - Official OpenAI SDK
- [AWS SDK for Go](https://github.com/aws/aws-sdk-go-v2) - Official AWS SDK

---

**Made with ‚ù§Ô∏è for the Go and AI community**

 [build-status-svg]: https://github.com/grokify/metallm/actions/workflows/ci.yaml/badge.svg?branch=main
 [build-status-url]: https://github.com/grokify/metallm/actions/workflows/ci.yaml
 [lint-status-svg]: https://github.com/grokify/metallm/actions/workflows/lint.yaml/badge.svg?branch=main
 [lint-status-url]: https://github.com/grokify/metallm/actions/workflows/lint.yaml
 [goreport-svg]: https://goreportcard.com/badge/github.com/grokify/metallm
 [goreport-url]: https://goreportcard.com/report/github.com/grokify/metallm
 [docs-godoc-svg]: https://pkg.go.dev/badge/github.com/grokify/metallm
 [docs-godoc-url]: https://pkg.go.dev/github.com/grokify/metallm
 [license-svg]: https://img.shields.io/badge/license-MIT-blue.svg
 [license-url]: https://github.com/grokify/metallm/blob/master/LICENSE
 [used-by-svg]: https://sourcegraph.com/github.com/grokify/metallm/-/badge.svg
 [used-by-url]: https://sourcegraph.com/github.com/grokify/metallm?badge
